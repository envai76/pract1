---
title: "practicum_1"
output:
  pdf_document: default
  html_document: default
date: "2023-05-19"
---

## loading the libraries

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(CatEncoders)
library(readr)          # Data Input
library(tidymodels)     # Data Manipulation
library(lubridate)      # Data Manupulation
library(dplyr)          # Data Manipulation
library(reshape2)       # Data Manipulation
library(caTools)        # Data Manipulation
library(corrplot)       # Data Visualisation
library(ggplot2)        # Data Visualization
library(viridis)        # Data Visualization
library(ggthemes)       # Data Visualization
library(pROC)           # Metrics
library(caret)          # Machine Learning
library(xgboost)        # xgboost model
```

## Understanding the data

**1.** Use the data dictionary describe each of the variables/features in the CSV in your report.

showing the data dictionary :

```{r}
data <- read.delim(file = "C:/Users/Narges/Documents/practical_1/data_dic.txt" ,  header = TRUE, sep = "\t", dec = ".")

print(data)
```

**2.** Can you think of 3 hypotheses for why someone may be more likely to miss a medical appointment?

Maybe the patient

**3.** Can you provide 3 examples of important contextual information that is missing in this data dictionary and dataset that could impact your analyses e.g., what type of medical appointment does each `AppointmentID` refer to?

1.how far is the patient address from the vitoria? 2.if the patient has any body to assist him or her? 3.the perso whom makes the appointment is the patient or some one else?

## Data Parsing and Cleaning

**4.** Modify the following to make it reproducible i.e., downloads the data file directly from version control

we can download data separately and import it from the saved address (line 41) or download it by the command in line 42( reproducibility)

```{r}
raw.data <- readr::read_csv('https://maguire-lab.github.io/health_data_science_research_2023/static_files/practicals/lab1_data/2016_05v2_VitoriaAppointmentData.csv')

print(raw.data)
```

checking if anybody is elder than 100 years or no?

```{r}
age_under_110 <- raw.data %>% filter(Age > 110)
print(age_under_110)
age_under_110 <- age_under_110[!duplicated( age_under_110[c('PatientID')]),]
print(age_under_110)

```

we can see that there are 2 person elder than 110.

## Exploratory Data Analysis

**5** Are there any individuals with impossible ages? If so we can drop this row using `filter` i.e., `data <- data %>% filter(CRITERIA)`

if the data contains a person with age under 0, so we can say that's impossible.

```{r}
raw.data %>% filter(Age<0)

```

as you can see we have one person under 0 age, so we should drop it from our data set. so I'm going to filter people with less than 0 age. in the below cell I'm going to check the data dimension before and after the filtering to see it has affected the data set or not.

```{r}
print(dim(raw.data))
raw.data <- raw.data %>% filter(Age >= 0)
print(dim(raw.data))
```

as its obvious just that one person is deleted.

so lets check if we have anybody with age 0 ?

```{r}
age_0 <- raw.data %>% filter(Age == 0)

age_0 <- age_0[!duplicated( age_0[c('PatientID')]),]
print(dim(age_0))

```

so we see that we have 2082 users with age 0 in our data set and i think that may happen for example newborn babies. so I'm not going to drop them.

but we are going to do some Exploratory on it. for example we wouldn't expect any of newborns to be diagnosed with Diabetes, Alcohol Use Disorder, and Hypertension. so we should check it in the data set too.

```{r}
raw.data %>% filter(Age == 0) %>% select(Hypertension, Diabetes, AlcoholUseDisorder) %>% unique()

```

We can also explore things like how many different neighborhoods are there and how many appoints are from each?

```{r}
count(raw.data, Neighbourhood, sort = TRUE)

```

**6** What is the maximum number of appointments from the same patient?

```{r}
count(raw.data, PatientID, sort = TRUE)

```

as you can see the maximum value equals to 88. i faced to a concern. and that's it, as we don't know whats the appointment for, if a person has more than one appointment in a day, it's a kind of duplicated data and i think they must be dropped out. so i would print the maximum appointment of a person also after deleting duplicates.

```{r}
raw.data <- raw.data [!duplicated( raw.data[c('PatientID','ScheduledDate')]),]

print(dim(raw.data))
count(raw.data, PatientID, sort = TRUE)

```

again the maximum is 88.

Let's explore the correlation between variables: first let's replace non numeric values with some numeric values(because with its real labels the correlation haetmap dosent represnt as expected). so replacements would be as follow:

1.for gender encoding:

```{r}
labs = LabelEncoder.fit(raw.data$Gender)

#convert labels to numeric values
raw.data$Gender = transform(labs, raw.data$Gender)
```

2.for Neighbourhood encoding:

```{r}
labs = LabelEncoder.fit(raw.data$Neighbourhood)

#convert labels to numeric values
raw.data$Neighbourhood = transform(labs, raw.data$Neighbourhood)
```

3.for NoShow encoding:

```{r}
labs = LabelEncoder.fit(raw.data$NoShow)

#convert labels to numeric values
raw.data$NoShow = transform(labs, raw.data$NoShow)
```

and after all checking the changes.

```{r}
print(raw.data)
```

Let's explore the correlation between variables:

```{r}
# let's define a plotting function
corplot = function(df){
  
  cor_matrix_raw <- round(cor(df),2)
  cor_matrix <- melt(cor_matrix_raw)
  
  
  #Get triangle of the correlation matrix
  #Lower Triangle
  get_lower_tri<-function(cor_matrix_raw){
    cor_matrix_raw[upper.tri(cor_matrix_raw)] <- NA
    return(cor_matrix_raw)
  }
  
  # Upper Triangle
  get_upper_tri <- function(cor_matrix_raw){
    cor_matrix_raw[lower.tri(cor_matrix_raw)]<- NA
    return(cor_matrix_raw)
  }
  
  upper_tri <- get_upper_tri(cor_matrix_raw)
  
  # Melt the correlation matrix
  cor_matrix <- melt(upper_tri, na.rm = TRUE)
  
  # Heatmap Plot
  cor_graph <- ggplot(data = cor_matrix, aes(Var2, Var1, fill = value))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "darkorchid", high = "orangered", mid = "grey50", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal()+ 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                     size = 8, hjust = 1))+
    coord_fixed()+ geom_text(aes(Var2, Var1, label = value), color = "black", size = 2) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank())+
      ggtitle("Correlation Heatmap")+
      theme(plot.title = element_text(hjust = 0.5))
  
  cor_graph
}

numeric.data = mutate_all(raw.data, function(x) as.numeric(x))

# Plot Correlation Heatmap
corplot(numeric.data)

```

**7** Which parameters most strongly correlate with missing appointments (`NoShow`)?

column SMSReceived is most related one to NoShow.

**8** Are there any other variables which strongly correlate with one another?

i would say the correlation by decending order as follow:

1.PatientID and AppointmentID are most related columns as their value in correlation heat map is equal to 0.65.

2.then AppointmentID and AppointmentDate are most correlated ones. also ScheduledDate and AppointmentDate are correlated with same corelation values(0.61).

3.third rank would be for Hypertension and Age with correlation value 0.5.

**9** Do you see any issues with PatientID/AppointmentID being included in this plot?

actually they are not carrying any important feature or information in themselves and they are just 2 ID columns. so their correlation don't make any help at the final predictions. because they are unique and they don't have any valuable information in themselves. Including PatientID and AppointmentID in a correlation plot may not provide meaningful insights as they are unique identifiers and not directly related to the variables being analyzed. It would be more appropriate to focus on numerical variables and relevant categorical variables for correlations.

Let's look at some individual variables and their relationship with NoShow.

```{r}
raw.data$NoShow = inverse.transform(labs, raw.data$NoShow)
print(raw.data)
ggplot(raw.data) + 
  geom_density(aes(x=Age, fill=NoShow), alpha=0.8) + 
  ggtitle("Density of Age by Attendence")
```

Let's take a closer look at age by breaking it into categories.

```{r}
raw.data <- raw.data %>% mutate(Age.Range=cut_interval(Age, length=10))

ggplot(raw.data) + 
  geom_bar(aes(x=Age.Range, fill=NoShow)) + 
  ggtitle("Amount of No Show across Age Ranges")

ggplot(raw.data) + 
  geom_bar(aes(x=Age.Range, fill=NoShow), position='fill') + 
  ggtitle("Proportion of No Show across Age Ranges")
```

**10.** How could you be misled if you only plotted 1 of these 2 plots of attendance by age group? The key takeaway from this is that number of individuals \> 90 are very few from plot 1 so probably are very small so unlikely to make much of an impact on the overall distributions. However, other patterns do emerge such as 10-20 age group is nearly twice as likely to miss appointments as the 60-70 years old.

```{r}
raw.data %>% filter(Age == 0) %>% count()


ggplot(raw.data) + 
  geom_bar(aes(x=SMSReceived, fill=NoShow), alpha=0.8) + 
  ggtitle("Density of SMS received across Age and No Show")

ggplot(raw.data) + 
  geom_bar(aes(x=SMSReceived, fill=NoShow), position='fill', alpha=0.8) + 
  ggtitle("Density of SMS received across Age and No Show")
```

**11.**From this plot does it look like SMS reminders increase or decrease the chance of someone not attending an appointment? Why might the opposite actually be true (hint: think about biases)?

it seems that reviving a sms has increased the chance of some one not attending to the appointment. i think the portion of received messages are lower than not received and it shows in misled way that its in opposite way. but actually by taking a look at the first plot i think it says if they have not recived that sms but they attended. and in the small portion of recived ones in compare to not recived ones, it works opposit.

**12.**Create a similar plot which compares the the density of `NoShow` across the values of disability

```{r}
ggplot(raw.data) + 
  geom_bar(aes(x=Disability, fill=NoShow), alpha=0.8) + 
  ggtitle("Density of SMS received across Age and No Show")

ggplot(raw.data) + 
  geom_bar(aes(x=Disability, fill=NoShow), position='fill', alpha=0.8) + 
  ggtitle("Density of Disability across Age and No Show")
```

Now let's look at the neighbourhood data as location can correlate highly with many social determinants of health.

```{r}
ggplot(raw.data) + 
  geom_bar(aes(x=Neighbourhood, fill=NoShow)) + 
  theme(axis.text.x = element_text(angle=45, hjust=1, size=5)) + 
  ggtitle('Attendance by Neighbourhood')

ggplot(raw.data) + 
  geom_bar(aes(x=Neighbourhood, fill=NoShow), position='fill') + 
  theme(axis.text.x = element_text(angle=45, hjust=1, size=5)) + 
  ggtitle('Proportional Attendance by Neighbourhood')
```

Most neighborhoods have similar proportions of no-show but some have much higher and lower rates.

**13** Suggest a reason for differences in attendance rates across neighbourhoods.

it seems some of them are so far and this far distance may cause this noshow.

Now let's explore the relationship between gender and NoShow.

```{r}
ggplot(raw.data) + 
  geom_bar(aes(x=Gender, fill=NoShow))
  ggtitle("Gender by attendance")
  
ggplot(raw.data) + 
  geom_bar(aes(x=Gender, fill=NoShow), position='fill')
  ggtitle("Gender by attendance")

```

**14** Create a similar plot using `SocialWelfare`

```{r}
ggplot(raw.data) + 
  geom_bar(aes(x=SocialWelfare, fill=NoShow))
  ggtitle("Gender by attendance")
  
ggplot(raw.data) + 
  geom_bar(aes(x=SocialWelfare, fill=NoShow), position='fill')
  ggtitle("Gender by attendance")
```

Far more exploration could still be done, including dimensionality reduction approaches but although we have found some patterns there is no major/striking patterns on the data as it currently stands.

However, maybe we can generate some new features/variables that more strongly relate to the `NoShow`.

## Feature Engineering

Let's begin by seeing if appointments on any day of the week has more no-show's. Fortunately, the `lubridate` library makes this quite easy!

```{r}
raw.data <- raw.data %>% mutate(AppointmentDay = wday(AppointmentDate, label=TRUE, abbr=TRUE), 
                                 ScheduledDay = wday(ScheduledDate,  label=TRUE, abbr=TRUE))

ggplot(raw.data) +
  geom_bar(aes(x=AppointmentDay, fill=NoShow)) +
  ggtitle("Amount of No Show across Appointment Day") 

ggplot(raw.data) +
  geom_bar(aes(x=AppointmentDay, fill=NoShow), position = 'fill') +
  ggtitle("Amount of No Show across Appointment Day") 

```

Let's begin by creating a variable called `Lag`, which is the difference between when an appointment was scheduled and the actual appointment.

```{r, fig.align="center"}
raw.data <- raw.data %>% mutate(Lag.days=difftime(AppointmentDate, ScheduledDate, units = "days"),
                                Lag.hours=difftime(AppointmentDate, ScheduledDate, units = "hours"))
raw.data <- raw.data %>% filter(Age >= 0)

ggplot(raw.data) + 
  geom_density(aes(x=Lag.days, fill=NoShow), alpha=0.7)+
  ggtitle("Density of Lag (days) by attendance")
```

**15** i thought that if the lag be a longer time, so the patient may have forgotten to attend but we see thay have attended. and i think may be the count of these late schecduled apointments are few.

## Predictive Modeling

Let's see how well we can predict NoShow from the data.

We'll start by preparing the data, followed by splitting it into testing and training set, modeling and finally, evaluating our results. For now we will subsample but please run on full dataset for final execution.

```{r}

###reading the data from scratch because i had made label encoding which its better to flash it back:
raw.data <- readr::read_csv('https://maguire-lab.github.io/health_data_science_research_2023/static_files/practicals/lab1_data/2016_05v2_VitoriaAppointmentData.csv')

print(raw.data)
### REMOVE SUBSAMPLING FOR FINAL MODEL
data.prep <- raw.data %>% select(-AppointmentID, -PatientID) #%>% sample_n(10000)

set.seed(42)
data.split <- initial_split(data.prep, prop = 0.7)
train  <- training(data.split)
test <- testing(data.split)
```

Let's now set the cross validation parameters, and add classProbs so we can use AUC as a metric for xgboost.

```{r}
fit.control <- trainControl(method="cv",number=3,
                           classProbs = TRUE, summaryFunction = twoClassSummary)
```

**16** Based on the EDA, how well do you think this is going to work?

Now we can train our XGBoost model.

```{r}
xgb.grid <- expand.grid(eta=c(0.05),
                       max_depth=c(4),colsample_bytree=1,
                       subsample=1, nrounds=500, gamma=0, min_child_weight=5)

xgb.model <- train(NoShow ~ .,data=train, method="xgbTree",metric="ROC",
                  tuneGrid=xgb.grid, trControl=fit.control)


```

again for changing the null levels on Neighbourhood to its real ones for both train and test.

```{r}
train$Neighbourhood<- as.factor(train$Neighbourhood)

levels(train$Neighbourhood)

test$Neighbourhood<- as.factor(test$Neighbourhood)

levels(test$Neighbourhood)
```

as we can see in above the levels of Neighbourhood in both test and train differ from each other. it happens because of splitting non shuffled data. so may be a portion including a special level goes to test portion which has not appeared in train. so we have to delete the difference to see the performance by confusion matrix.

```{r}
test <- subset(test,Neighbourhood !=  "PARQUE INDUSTRIAL")

```

because the levels of test\$NoShow is null i have to do the following code at first:

```{r}
test$NoShow<- as.factor(test$NoShow)
```

then lets check the XGBoost model performance:

```{r}
xgb.pred <- predict(xgb.model, newdata=test)
xgb.probs <- predict(xgb.model, newdata=test, type="prob")

test <- test %>% mutate(NoShow.numerical = ifelse(NoShow=="Yes",1,0))
confusionMatrix(xgb.pred, test$NoShow, positive="Yes")
paste("XGBoost Area under ROC Curve: ", round(auc(test$NoShow.numerical, xgb.probs[,2]),3), sep="")
```

This isn't an unreasonable performance, but let's look a bit more carefully at the correct and incorrect predictions,

```{r ,fig.align="center"}
xgb.probs$Actual = test$NoShow.numerical
xgb.probs$ActualClass = test$NoShow
xgb.probs$PredictedClass = xgb.pred
xgb.probs$Match = ifelse(xgb.probs$ActualClass == xgb.probs$PredictedClass,
                         "Correct","Incorrect")
# [4.8] Plot Accuracy
xgb.probs$Match = factor(xgb.probs$Match,levels=c("Incorrect","Correct"))
ggplot(xgb.probs,aes(x=Yes,y=Actual,color=Match))+
  geom_jitter(alpha=0.2,size=0.25)+
  scale_color_manual(values=c("grey40","orangered"))+
  ggtitle("Visualizing Model Performance", "(Dust Plot)")
```

Finally, let's close it off with the variable importance of our model:

```{r,fig.align="center"}
results = data.frame(Feature = rownames(varImp(xgb.model)$importance)[1:10],
                     Importance = varImp(xgb.model)$importance[1:10,])

results$Feature = factor(results$Feature,levels=results$Feature)


# [4.10] Plot Variable Importance
ggplot(results, aes(x=Feature, y=Importance,fill=Importance))+
  geom_bar(stat="identity")+
  scale_fill_gradient(low="grey20",high="orangered")+
  ggtitle("XGBoost Variable Importance")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**17** Using the [caret package](https://topepo.github.io/caret/) fit and evaluate 1 other ML model on this data.

## Fitting Vanilla Neural Network

```{r}
### REMOVE SUBSAMPLING FOR FINAL MODEL
data.prep <- raw.data %>% select(-AppointmentID, -PatientID) #%>% sample_n(1000)

set.seed(42)
data.split <- initial_split(data.prep, prop = 0.7)
train  <- training(data.split)
test <- testing(data.split)

fit.control <- trainControl(method="cv",number=3,
                           classProbs = TRUE, summaryFunction = twoClassSummary)
```

```{r}
set.seed(123)
neural.network <- train( NoShow ~., 
                         data = train,
                         method  = "nnet",
                         na.action = na.omit,
                         trControl = fit.control,
                         trace = FALSE)
```

again for changing the null levels on Neighbourhood to its real ones for both train and test.

```{r}
train$Neighbourhood<- as.factor(train$Neighbourhood)

levels(train$Neighbourhood)

test$Neighbourhood<- as.factor(test$Neighbourhood)

levels(test$Neighbourhood)
```

as we can see in above the levels of Neighbourhood in both test and train differ from each other. it happens because of splitting non shuffled data. so may be a portion including a special level goes to test portion which has not appeared in train. so we have to delete the difference to see the performance by confusion matrix.

```{r}
test <- subset(test,Neighbourhood !=  "PARQUE INDUSTRIAL")

```

also same as before we have to use the following code to change the levels of test\$Noshow from null to its real levels.

```{r}
test$NoShow<- as.factor(test$NoShow)
```

now lets check the outputs and the performance:

```{r}
# Best tuning parameter mtry
neural.network$bestTune
# Make predictions on the test data
predictions <- neural.network %>% predict(test)
head(predictions)
head(test$NoShow)

```

```{r}

nn.pred <- predict(neural.network, newdata=test)
nn.probs <- predict(neural.network, newdata=test, type="prob")

test <- test %>% mutate(NoShow.numerical = ifelse(NoShow=="Yes",1,0))
confusionMatrix(nn.pred, test$NoShow, positive="Yes")
paste("neural network Area under ROC Curve: ", round(auc(test$NoShow.numerical, nn.probs[,2]),3), sep="")
```

**18** Based on everything, do you think we can trust analyses based on this dataset? Explain your reasoning.

actually the percision is not too high, so we can not trust with high level to this models. The provided models have low accuracy and perform poorly in identifying positive instances. Their sensitivity is extremely low, and in the case of the second model, it fails to identify any positive instances at all. Therefore, we cannot trust the analyses based on these models as their performance is not reliable.

For the first model, the accuracy is 0.8023, which means it correctly classifies approximately 80.23% of the instances. However, when we examine the sensitivity (also known as recall or true positive rate), it is extremely low at 0.030848, indicating that the model has a poor ability to correctly identify positive instances (Yes). The specificity (true negative rate) is relatively high at 0.994462, suggesting a good ability to correctly identify negative instances (No). However, the low sensitivity severely impacts the overall performance of the model.

Similarly, the second model has an accuracy of 0.8006, which is slightly lower than the first model. However, the sensitivity is 0.0000, indicating that the model fails to identify any positive instances correctly. This is further supported by the fact that the positive predictive value is NaN (not a number), which indicates that the model does not make any positive predictions at all. The specificity remains at 1.0000, meaning the model correctly identifies all negative instances.
